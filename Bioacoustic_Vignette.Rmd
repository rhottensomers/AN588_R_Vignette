---
title: "Bioacoustic Vignette"
author: "Reese Hotten-Somers, Ritika Sibal, Sofia Weaver, Paige Becker, Allister Malik"
date: "2023-11-23"
output:  
    html_document:
    theme: cosmo
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preliminaries: 
Install these packages in *R*:{bioacoustics}, {warbleR}, {randomForest}, {tools}, {viridis}

## Objectives: 
The objective of this module is to introduce you to analysis of biacoustic data so that you can read, display, and analyze audio recordings of vocalizations in animals.

## Intro to Bioacoustics:

What is bioacoustics, who uses it, and how is it implemented? 

Broadly, bioacoustics is the the noninvasive crossdisciplinary science of biology and acoustics that studies the production, transmission, and reception of animal sounds. Given the wide range of information that bioacoustics provides, it's used by researchers, physicians, and government agencies. Studying sounds and it's affects on living organisms, can provide powerful insight into natural history, structure and dynamics of ecosystems, species biodiversity, habitat and species health, information climate change, and much more.  

### Typical Frequency Ranges:

Different Sounds will happen at different frequencies. This is important to know because you settings may have to change in order to display and analyze your bioacoustic data correctly. Here are a general list of frequency ranges that you might encounter:

* Humans: 100 Hz to 900 Hz
* Apes: 900 Hz to 1,400 Hz
* Birds: 1,000 Hz to 8,000 Hz 
* Bats: 9,000 Hz to 200,000 Hz
* Frogs: 1500 Hz to 6000 Hz
* Whales: 30 Hz to 8,000 Hz
* Dolphins: 200 Hz to 150,000 Hz

First, let's load in the packages that we need:
## Load in Packages
```{r, prelims}
library(warbleR)
library(bioacoustics)
library(tools)
library(randomForest)
# Unfortunately, ohun is currently under policy violation, so for the time being we are using an archived version. We expect this library to be unarchived after investigation.
url <- "https://cran.r-project.org/src/contrib/Archive/ohun/ohun_1.0.0.tar.gz"
install.packages(url, type="source", repos=NULL)
library(ohun)
library(viridis)
```

## Importing Data from the internet

The Warblr package comes with the function query_xc which allows you to seamlessly import audio data from xeno-canto[https://xeno-canto.org/484847].
```{r, importing}
#pulling all Strepera versicolor calls without downloading locally
df1 = query_xc(qword ='Strepera versicolor type:call cnt:"Australia"', download = FALSE) #todo: include explanation for parameters of query_xc

#Filter by type   
df1 = df1[df1$Vocalization_type=="call",] #filtering by only calls
df1 = df1[df1$Quality=="A",] #a type quality
df1 = df1[1:9,]#takes the first 9 mp3 files 
View(df1)

#Download data to working directory
query_xc(X = df1, download = TRUE, path = getwd())

```

## Importing Your Own Data

Great! Now that we know how to load in data from an cloud-based data set, lets try something that will probably be more useful in your own future research endeavor. Say you are a ecologist who has just come back from a field expedition where you set up passive bio-acoustic monitors. All the recorded audio files (usually in .mp3 or .wav formats) are saved in a harddrive. How would you import your data then?    

You can import you files straight from your Harddrive, Desktop or anywhere else on your personal computer. No need for cloud storage! The following code reads in a file named *mgr.wav* which, for the purpose of this example, has been saved in your working directory. 

```{r, importing_mp3}
# getwd() gets our current working directory and file.path() combines getwd() and "mgr.wav" into a file path format. 
wav <- read_audio(file.path(getwd(), "mgr.wav"))

# Say you have a file saved on your Desktop, your code might look a little like this:
# wav <- read_audio(file.path("~/Desktop", "mgr.wav"))
```

![pied currawong](pied-currawong.png)

Now that we know how to read files, lets load in a file named "Strepera-versicolor-571957.mp3". Here is the mp3 file:
```{r, results='asis', echo=FALSE}
cat('<audio controls> 
  <source src="Strepera-versicolor-571957.mp3" type="audio/mpeg">
</audio>') #html code
```

While reading in one file at a time is helpful, an ecologist returning from a month long expedition will surely have more than one file to read. We have written a funciton here that will read all the mp3 files in your working directory. You could expand on this funciton to read files in different directories or even create a data frame!

```{r, readf}
# Read just one file!
STREPV <- read_audio(file.path(getwd(), "Strepera-versicolor-571957.mp3"))
STREPV


# Read all the files in your current directory!
read_all_mp3 <- function(){
  # List all mp3 files in current directory
  mp3_files = list.files(path=".", pattern=".mp3", all.files=TRUE, 
                         full.names=TRUE)
  audio_list <- list()
  # Append all the files a paths to a single list named audio_list
  for(files in mp3_files) {
    curr_file_path <- read_audio(file.path(files))
    audio_list <- append(audio_list, curr_file_path)
  }
  return(audio_list)
}

read_all_mp3()
```


## Plotting the data:

Now that we've imported, converted, and read our audio files it's time to plot them. This can be done many different ways depending on what information you need out of your bioacoustic information. These primlimary graph simply display the wave files but later on we will give you a way to graph analysis onto them. 


The simplest of the graphs is plot() which plots wave objects. It displays a black and white graph of audio spikes. This is a waveform,  x axis is the time of the file and the yaxis is a amplitude, or the displacement of a particle
```{r, plotp}
#first you call your wave audio file that we converted previously: 

plot(STREPV)

#Now let's zoom in on the first audio spike: 

plot(STREPV, xlim = c(3, 5)) # xlim is indicating the time limits to narrow the field of analysis
```

Plot can also be used with stereo audio files where there is audio from the right and left channel. To see if your data is stero vs mono you can look at the list from either read_audio or read_all_mp3(). Under the audio file it will state Channels(mono/stereo) and if it is stereo you can plot it this way: 
```{r, steroplot}
 #reads stereo audio and converts it into a wave object. 
STREPV2 <- read_audio(file.path(getwd(), "Strepera-versicolor-743906.mp3")) 

#displays wave object information. You can see clearly here that STREPV2 is a stereo wave object
STREPV2

#now let's plot it!
plot(STREPV2)
```

This plot graph is similar to the one above but it has one graph for the right channel and one for the left. Stero and mono wave object do plot differently with plot() so always make sure to confirm what kind of channel your object is. 

While plot() is wonderful spectro() and fspec() are both plotting devices that come with the bioacoustics r package. They are able to generate spectrograms. Spectrograms are a visual representation of the strength or loudness of the audio signal/frequency throughout the wave object. Spectro, true to its name, plots a spectrogram of your data. 
```{r, spectro}
 #First let's look at the parameters within spectrogram:
?spectro()

#tick indicates the frequency tick marks. We are displaying from 1 to 20 khz by 1khz steps. Different animals may need different frequencies so be sure to check that you are display the correct range. 
ticks <- c(from = 1000, to = 20000, by = 1000)

#Specify the time limits on the X-axis in seconds (s)
temp_slice <- c(from = 1, to = 10)

spectro(STREPV, tlim = temp_slice, FFT_size = 512, ticks_y = ticks) #sets the y axis labeling, in Hz, so its 1 kHz to 100 kHz, in 1 kHz interval
```
NOTE: Y axis is frequency in hertz and X axis is time. However spectro() does not have the ability to add x axis label so it should only be used for visualization. 
Spectro() is wonderful for an elementary spectrogram but there is a better way to visualize the data with fspec(). It is a matrix of amplitude or decibel (dB) values in the time / frequency domain. This function on its own generates only the matris of a spectrogram so in order to visual the data we use image() to display the graph. 

```{r, fspec}
#First let's look at the parameters:
?fspec

#Now graph!
image(fspec(STREPV, tlim = temp_slice, rotate = TRUE)) 
```

NOTE: If you have older zero-crossing bat vocalization patterns, then the functions write_zc() and plot_zc() will serve as replacements for fspec() and spectro(). (Add in paragraph after running through spectro and fspec() with an asterisk)

## Plotting parts of the data:

Now that we have the prepared audiofile in its manipulatative form, we can filter the desired sounds(the bird calls) from background noise(other bird noises, self noise, etc).

Blob_detection is one function that detects and filters audio and displays them in batches. 
It is a common type of detection/filtration algorithm used not only in audio analysis, but also image processing. Blob detection can be used to filter and identify colors, intensity/brightness etc. 

```{r blob_detec}
# blob_detection
#?blob_detection 
blob_detection(
  STREPV, #the waveform of the audio file made
  time_exp = 1, #Time expansion factor, 1 represents real-time
  min_dur = 44, #Max duration of the audio event, in ms
  max_dur =1500,  #Max duration of the audio event, in ms, based on the plot(1500 ms) is the average length of the birds call
  min_area = 100, #Minimum number of pixels in the area of the audio event
  min_TBE = 20, #Min window between 2 audio events
  max_TBE = 10000,
  LPF = 25000, #Low-Pass Filter. Frequencies above the cutoff are not included.
          HPF = 2000,
          FFT_size = 256,# Size of the Fast Fourier Transform (FFT) window
          FFT_overlap = 0.875, #Percentage of overlap between two FFT windows
          blur = 2, #Gaussian smoothing function for blurring the spectrogram of the audio event to reduce image noise
          spectro_dir = file.path(getwd(), "Spectros"), #Path to the filtered spectrogram png files used on the yielded HTML
          time_scale = 2, #Time resolution of the spectrogram in milliseconds (ms) per pixel (px). 
)
```
The function returns an HTML file which shows all the audio events that fall within the parameters of the function


Threshold detection is another form of detection that identifies events based on the SNR, the signal-to-noise ratio. If the amplitude is above the SNR, an audio event is recorded.
```{r, plot_data}
# blob_detection
?blob_detection
blob_detect <- blob_detection(
  STREPV, time_exp = 10, FFT_size = 512, settings = FALSE, acoustic_feat = TRUE,
  metadata = FALSE, spectro_dir = file.path(getwd(), "Spectros"), time_scale = 0.1, ticks = TRUE)
blob_detect

# Threshold_detection
?threshold_detection
TD <- threshold_detection(
  STREPV, 
  threshold = 12, #threshold sets the sensitivity of the event, compared to the SNR(in dB)
  time_exp = 1,
  min_dur = 44,
  max_dur = 1500, 
  min_TBE = 20, 
  max_TBE = 10000, 
  LPF = 25000, 
  HPF = 2000, 
  start_thr = 20, 
  end_thr = 30, 
  SNR_thr = 8, #SNR threshold (dB) at which the extraction of the audio event stops.
  angle_thr = 125, #Angle threshold (°) at which the audio event extraction stops.
  duration_thr = 500, 
  NWS = 1000, #Length of time used for background noise estimation in the recording (ms)
  KPE = 1e-05, #Set the Process Error parameter of the Kalman filter. 
  KME = 1e-04, #Set the Measurement Error parameter of the Kalman filter.
  spectro_dir = file.path(getwd(), "Spectros"), 
  time_scale = 2, 
)
TD
```

The auto_detec function automatically detects the start and end of vocalizations in sound files based on amplitude, duration, and frequency range attributes. 
```{r,autodetec}
#let's look at the parameters of the function first"
?auto_detec
detection <- auto_detec(threshold = 5, ssmooth = 300, bp = c(2, 9), wl = 300, path = getwd())
detection

detection$sound.files
```


The packages ohun and viridis allow us to visualize the autodetection points determined above plotted on a spectrogram.

```{r, ohun}
label_spectro(wave = STREPV, detection = detection[detection$sound.files == "Strepera-versicolor-571957.mp3", ], hop.size = 10, ovlp = 50, flim = c(1, 10), fastdisp=TRUE) 
```

## *CHALLENGE*
1. Record an audio file of you saying something (around 30 seconds!)
2. Transfer that file onto your laptop. Save it whereever you'd like!
3. Read in the mp3 file. 
4. Plot the spectrogram from 10-20 seconds of the audio file.
5. Run a detection algorithm on your file and plot the detection results.

``` {r Challenge Code}
#wav <- read_audio(file.path("~/Desktop", "your_file_name.wav"))
#temp_slice <- c(from = 10, to = 20) # in seconds
#ticks <- c(from = 1000, to = 20000, by = 1000)
#spectro(wav, tlim = temp_slice, FFT_size = 512, ticks_y = ticks) #sets the y axis labeling, in Hz, so its 1 kHz to 100 kHz, in 1 kHz interval
#detection <- auto_detec(threshold = 5, ssmooth = 300, bp = c(2, 9), wl = 300, path = getwd())

#label_spectro(wave = wav, detection = detection[detection$sound.files == "mgr.wav", ], hop.size = 10, ovlp = 50, flim = c(1, 10), fastdisp=TRUE) 
```
