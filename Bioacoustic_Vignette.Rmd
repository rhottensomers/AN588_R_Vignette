---
title: "Bioacoustic Vignette"
author: "Reese Hotten-Somers"
date: "2023-10-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TO DO by Teusday morning:
Reese will make the file pretty (everyone will say when they are done)
Sofia: Add pictures (of the bird we are analyzing)
Paige: Typical frequency ranges for humans, apes, birds, bats, frogs, and whales/dolphins
Allister: Add the mp3 file that is playable to the markdown
Ritika: Deep learning

##Preliminaries: 
Install these packages in *R*:{bioacoustics}, {warbleR}, {randomForest}, {Rraven}

## Objectives: 
The objective of this module is to introduce you to analysis of biacoustic data so that you can read, display, and analyze audio recordings of vocalizations in animals.

## Intro to Bioacoustics:

What is bioacoustics, who uses it, and how is it implemented? 

Broadly, bioacoustics is the the noninvasive crossdisciplinary science of biology and acoustics that studies the production, transmission, and reception of animal sounds. Given the wide range of information that bioacoustics provides, it's used by researchers, physicians, and government agencies. Studying sounds and it's affects on living organisms, can provide powerful insight into natural history, structure and dynamics of ecosystems, species biodiversity, habitat and species health, information climate change, and much more.  

### Typical Frequency Ranges:
* Humans: 100 Hz to 900 Hz
* Apes: 900 Hz to 1,400 Hz
* Birds: 1,000 Hz to 8,000 Hz 
* Bats: 9,000 Hz to 200,000 Hz
* Frogs: 1500 Hz to 6000 Hz
* Whales: 30 Hz to 8,000 Hz
* Dolphins: 200 Hz to 150,000 Hz

## Load in Packages
```{r, prelims}
library(warbleR)
library(bioacoustics)
library(tools)
library(randomForest)
# Unfortunately, ohun is currently under policy violation, so for the time being we are using an archived version. We expect this library to be unarchived after investigation.
url <- "https://cran.r-project.org/src/contrib/Archive/ohun/ohun_1.0.0.tar.gz"
install.packages(url, type="source", repos=NULL)
library(ohun)
library(viridis)
```

# Importing Data from the internet
Warblr comes with the function query_xc which allows you to seamlessly import audio data from xeno-canto[https://xeno-canto.org/484847].
```{r, importing}
#pulling all Strepera versicolor calls without downloading locally
df1 = query_xc(qword ='Strepera versicolor type:call cnt:"Australia"', download = FALSE) #todo: include explanation for parameters of query_xc

#Filter by type   
df1 = df1[df1$Vocalization_type=="call",]
df1 = df1[df1$Quality=="A",]
df1 = df1[1:9,]
View(df1)

#Download data to working directory
query_xc(X = df1, download = TRUE, path = getwd())

```

# Importing Your Own Data

Great! Now that we know how to load in data from an cloud-based data set, lets try something that will probably be more useful in your own future research endeavor. Say you are a ecologist who has just come back from a field expedition where you set up passive bio-acoustic monitors. All the recorded audio files (usually in .mp3 or .wav formats) are saved in a harddrive. How would you import your data then?    

You can import you files straight from your Harddrive, Desktop or anywhere else on your personal computer. No need for cloud storage! The following code reads in a file names *mgr.wav* which, for the purpose of this example, has been saved in your working directory. 

```{r, importing_mp3}
# getwd() gets our current working directory and file.path() combines getwd() and "mgr.wav" into a file path format. 
wav <- read_audio(file.path(getwd(), "mgr.wav"))

# Say you have a file saved on your Desktop, your code might look a little like this:
# wav <- read_audio(file.path("~/Desktop", "mgr.wav"))
```

Now that we know how to read files, lets load in a file named "Strepera-versicolor-571957.mp3". 

While reading in one file at a time is helpful, an ecologist returning from a month long expedition will surely have more than one file to read. We have written a funciton here that will read all the mp3 files in your working directory. You could expand on this funciton to read files in different directories or even create a data frame!

```{r, readf}
# Read just one file!
STREPV <- read_audio(file.path(getwd(), "Strepera-versicolor-571957.mp3"))
STREPV


# Read all the files in your current directory!
read_all_mp3 <- function(){
  # List all mp3 files in current directory
  mp3_files = list.files(path=".", pattern=".mp3", all.files=TRUE, 
                         full.names=TRUE)
  audio_list <- list()
  # Append all the files a paths to a single list named audio_list
  for(files in mp3_files) {
    curr_file_path <- read_audio(file.path(files))
    audio_list <- append(audio_list, curr_file_path)
  }
  return(audio_list)
}

read_all_mp3()

# TO DO: DELETE THIS (I DONT THINK WE NEED IT)
#Reading a wave object: Duration, file length/durations
#Number of samples, number of intervals taken from the file
#Sample rate, the rate of each sample taken. Here's a nice visual[https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Signal_Sampling.svg/1920px-Signal_Sampling.svg.png]
  #You can find number of samples by multiplying the Samplingrate by Duration
#Channels: Is the audio recorded from one source or "channel"(mono) or multiple channels (stereo)
#PCM is a method used to "digitally represent sampled analog signals."
#Bit is the number of bit per sample
```


## Plotting:

Now that we've imported, converted, and read our audio files it's time to plot them. This can be done many different ways depending on what information you need out of your bioacoustic information. These primlimary graph simply display the wave files but later on we will give you a way to graph analysis onto them. 


The simplest of the graphs is plot() which plots wave objects. It displays a black and white graph of audio spikes. This is a waveform,  x axis is the time of the file and the yaxis is a amplitude, or the displacement of a particle
```{r, plotp}
#first you call your wave audio file that we converted previously: 

plot(STREPV)

#Now let's zoom in on the first audio spike: 

plot(STREPV, xlim = c(3, 5)) # xlim is indicating the time limits to narrow the field of analysis
```

Plot can also be used with stereo audio files where there is audio from the right and left channel. To see if your data is stero vs mono you can look at the list from either read_audio or read_all_mp3(). Under the audio file it will state Channels(mono/stereo) and if it is stereo you can plot it this way: 
```{r, steroplot}
 #reads stereo audio and converts it into a wave object. 
STREPV2 <- read_audio(file.path(getwd(), "Strepera-versicolor-743906.mp3")) 

#displays wave object information. You can see clearly here that STREPV2 is a stereo wave object
STREPV2

#now let's plot it!
plot(STREPV2)
```

This plot graph is similar to the one above but it has one graph for the right channel and one for the left. Stero and mono wave object do plot differently with plot() so always make sure to confirm what kind of channel your object is. 

While plot() is wonderful spectro() and fspec() are both plotting devices that come with the bioacoustics r package. They are able to generate spectrograms. Spectrograms are a visual representation of the strength or loudness of the audio signal/frequency throughout the wave object. Spectro, true to its name, plots a spectrogram of your data. 
```{r, spectro}
 #First let's look at the parameters within spectrogram:
?spectro()

#tick indicates the frequency tick marks. We are displaying from 1 to 20 khz by 1khz steps. Different animals may need different frequencies so be sure to check that you are display the correct range. 
ticks <- c(from = 1000, to = 20000, by = 1000)

#Specify the time limits on the X-axis in seconds (s)
temp_slice <- c(from = 1, to = 10)

spectro(STREPV, tlim = temp_slice, FFT_size = 512, ticks_y = ticks) #sets the y axis labeling, in Hz, so its 1 kHz to 100 kHz, in 1 kHz interval
```
NOTE: Y axis is frequency in hertz and X axis is time. However spectro() does not have the ability to add x axis label so it should only be used for visualization. 
Spectro() is wonderful for an elementary spectrogram but there is a better way to visualize the data with fspec(). It is a matrix of amplitude or decibel (dB) values in the time / frequency domain. This function on its own generates only the matris of a spectrogram so in order to visual the data we use image() to display the graph. 

```{r, fspec}
#First let's look at the parameters:
?fspec

#Now graph!
image(fspec(STREPV, tlim = temp_slice, rotate = TRUE)) 

#you can also change the color of the graph: 
image(fspec(STREPV, tlim = temp_slice, rotate = TRUE, col = hcl.colors(20, "viridis", rev = TRUE))) #This is not working rn we should fix it. 
```

NOTE: If you have older zero-crossing bat vocalization patterns, then the functions write_zc() and plot_zc() will serve as replacements for fspec() and spectro(). (Add in paragraph after running through spectro and fspec() with an asterisk)

# Plotting the data
Now that we have the prepared audiofile in its manipulatative form, we can filter the desired sounds(the bird calls) from background noise(other bird noises, self noise, etc).

Blob_detection is one function that detects and filters audio and displays them in batches. 
It is a common type of detection/filtration algorithm used not only in audio analysis, but also image processing. Blob detection can be used to filter and identify colors, intensity/brightness etc. 

```{r blob_detec}
# blob_detection
#?blob_detection 
blob_detect <- blob_detection(
  STREPV, #the waveform of the audiofile
  time_exp = 1, #Time expansion factor, 1 represents real time
  min_dur = 25, #min duration of an audio event to be recoreded, in ms
  max_dur = 1500, #Max duration of audio event, in ms, based on audio 1.5 seconds(1500 ms) is the average length of the birds call
  LPF = 8000, #Frequnecies above this cutoff are attenuated
  HPF = 1000, #Frequencies below the cutoff are greatly attenuated.
  min_TBE = 10,
  max_TBE = 10000,
  blur = 0,
  FFT_size = 256, #
  settings = FALSE, #simply does not save the parameters for the threshold_detection
  acoustic_feat = TRUE, 
  metadata = FALSE, #extracts metadata in a dataframe
  spectro_dir = file.path(getwd(), "Spectros"), #Gives directions of the folder containing the .png files
  ticks = TRUE) 
blob_detect
```
The function returns an HTML file which shows all the audio events that fall within the parameters of the function


Threshold detection is another form of detection that identifies events based on the SNR, the signal-to-noise ratio. If the amplitude is above the SNR, an audio event is recorded.
```{r, plot_data}
# blob_detection
?blob_detection
blob_detect <- blob_detection(
  STREPV, time_exp = 10, FFT_size = 512, settings = FALSE, acoustic_feat = TRUE,
  metadata = FALSE, spectro_dir = file.path(getwd(), "Spectros"), time_scale = 0.1, ticks = TRUE)
blob_detect

# Threshold_detection
?threshold_detection
TD <- threshold_detection(
  STREPV, 
  threshold = 12, #threshold sets the sensitivity of the event, compared to the SNR
  time_exp = 1,
  min_dur = 140, #min duration for 1 audio event
  max_dur = 440, #max duration for an audio event
  min_TBE = 10,  
  max_TBE = 5000, 
  EDG = 0.996, 
  LPF = 10000, 
  HPF = 1000, 
  FFT_size = 256, 
  FFT_overlap = 0.875, 
  start_thr = 20, 
  end_thr = 30, 
  SNR_thr = 8, 
  angle_thr = 125, 
  duration_thr = 440, 
  NWS = 1000, 
  KPE = 1e-05, 
  KME = 1e-04, 
  settings = FALSE, 
  acoustic_feat = TRUE, 
  metadata = FALSE, 
  spectro_dir = file.path(getwd(), "Spectros"), 
  time_scale = 1, 
  ticks = TRUE
)
TD


# auto_detec automatically detects the start and end of vocalizations in sound files based on amplitude, duration, and frequency range attributes.
# https://rdrr.io/cran/warbleR/man/auto_detec.html
?auto_detec
detection <- auto_detec(threshold = 5, ssmooth = 300, bp = c(2, 9), wl = 300, path = getwd())
detection

library(ohun)
library(viridis)

detection$sound.files
label_spectro(wave = STREPV, detection = detection[detection$sound.files == "Strepera-versicolor-571957.mp3", ], hop.size = 10, ovlp = 50, flim = c(1, 10), fastdisp=TRUE) 
```

# *CHALLENGE*
1. Record an audio file of you saying something (around 30 seconds!)
2. Transfer that file onto your laptop. Save it whereever you'd like!
3. Read in the mp3 file. 
4. Plot the spectrogram from 10-20 seconds of the audio file.
5. Run a detection algorithm on your file and plot the detection results.

``` {r Challenge Code}
wav <- read_audio(file.path("~/Desktop", "your_file_name.wav"))
temp_slice <- c(from = 10, to = 20) # in seconds
ticks <- c(from = 1000, to = 20000, by = 1000)
spectro(wav, tlim = temp_slice, FFT_size = 512, ticks_y = ticks) #sets the y axis labeling, in Hz, so its 1 kHz to 100 kHz, in 1 kHz interval
detection <- auto_detec(threshold = 5, ssmooth = 300, bp = c(2, 9), wl = 300, path = getwd())

label_spectro(wave = wav, detection = detection[detection$sound.files == "mgr.wav", ], hop.size = 10, ovlp = 50, flim = c(1, 10), fastdisp=TRUE) 
```
